# PITFALLS — dsys-tool

**Research dimension:** Pitfalls
**Question:** What do projects like this commonly get wrong across: (1) AI vision extraction, (2) design token generation, (3) multi-platform output, (4) Claude Code sub-agent orchestration?
**Date:** 2026-02-17

---

## Domain Summary

Design system generation tools that combine AI vision analysis with code generation fail in predictable ways. The failures cluster around four axes: trusting AI vision output without grounding it in constraints, designing tokens that don't translate across platforms, producing output that is technically correct but not "drop-in ready," and building agent pipelines that are brittle in ways that only surface at runtime. Each axis has a distinct failure mode and requires a specific prevention strategy.

---

## Pitfall 1: Treating AI Vision Output as Ground Truth

**Domain:** AI vision extraction from screenshots

**What goes wrong:** The analysis agent extracts a color like `#1A2B3C` from a screenshot. That value is real — it appeared in the image. But it was a shadow overlay on top of the true brand blue, a compression artifact, or a hover state captured mid-transition. Downstream token generation uses it as a primary color, and the entire palette is built on a visual accident. The same failure pattern applies to font sizes (screenshot scaling distorts them), spacing values (pixel-picked from a JPEG loses sub-pixel precision), and border radii (antialiasing makes curves ambiguous).

**Why it's worse than it looks:** Multiple analysis agents run in parallel, each examining a different benchmark. Each may pick a slightly different "blue" from their respective screenshot. The synthesizer reconciles `#1A2B3C`, `#1B2C3D`, and `#1A2C3C` — which are all compression variants of the same brand color — into three separate tokens, or worse, averages them into a value that matches none of the originals.

**Warning signs:**
- Color palette from synthesis contains more than ~8-10 distinct hues for a single brand
- Font sizes are non-round numbers (e.g., 14.3px, 17.8px) rather than values on a standard scale
- Spacing values don't cluster around multiples of 4 or 8
- Adjacent benchmarks from the same brand produce noticeably different primary colors
- Analysis agent prompts don't include explicit instructions to reason about what is "intentional design" vs. "rendering artifact"

**Prevention strategy:**
1. Constrain the vision extraction prompt to reason about *design intent*, not pixel measurement. Ask "what is the intended color system" not "what colors do you see."
2. Apply post-extraction quantization: snap colors to the nearest member of a candidate palette generated by k-means clustering across all extracted values. Snap font sizes to a standard type scale (6, 8, 10, 12, 14, 16, 18, 20, 24, 28, 32, 36, 48, 64...). Snap spacing to 4px grid multiples.
3. The synthesizer agent must flag when the same semantic role (e.g., "primary brand color") resolves to values with perceptual distance above a threshold. Force a decision rather than silently averaging.
4. Test the full pipeline against a known design system (e.g., feed in screenshots of Material Design or Tailwind UI, then compare extracted tokens against the published design system values).

**Phase:** Analysis agent design (before writing any extraction prompts), Synthesizer agent design

---

## Pitfall 2: Extracting Colors Without Extracting Semantics

**Domain:** AI vision extraction + token generation

**What goes wrong:** The analysis produces a list of hex values: `#0F172A`, `#3B82F6`, `#EFF6FF`, `#F59E0B`, `#EF4444`. These are real colors from the benchmarks. But without semantic labels — which is the background, which is the primary action, which is a warning, which is a destructive action — the token generator can only produce `color-1` through `color-5`. The consumer (a future Claude Code session) has no idea which token to use for a button vs. a badge vs. an alert.

**The deeper problem:** Semantic roles can't be reliably inferred from color value alone after the fact. The mapping must happen *during* extraction, while the AI has the visual context of where each color appears and what it does. Once that context is lost, it cannot be reconstructed.

**Warning signs:**
- Token output uses positional names (`color-1`, `gray-3`) rather than semantic names (`color-background`, `color-action-primary`, `color-feedback-danger`)
- The analysis prompt asks "what colors exist" rather than "what is each color used for"
- Generated token file has no description fields, only values
- CLAUDE.md rules generated from the tokens reference raw hex values rather than token names

**Prevention strategy:**
1. Structure the extraction prompt as a semantic inventory, not a color picker. For each color the agent identifies, require it to simultaneously classify: role (background/surface/border/text/interactive/feedback), state context (default/hover/active/disabled), and prominence (primary/secondary/tertiary).
2. Define a fixed semantic schema before writing any extraction prompts. Every benchmark analysis must produce values keyed to the same schema. The synthesizer merges same-keyed values across benchmarks.
3. Token output format must include `description` fields that explain the intent ("Use for primary action buttons and interactive links, not decorative elements").
4. Integration test: pass the token output to a fresh Claude Code session and ask it to build a simple form. If it reaches for the wrong token for a button, the semantic labeling failed.

**Phase:** Token schema design (must happen before any code is written), Analysis agent prompt design

---

## Pitfall 3: Synthesis Loses the Aesthetic Signal

**Domain:** AI vision extraction + synthesis across multiple benchmarks

**What goes wrong:** The synthesizer merges token values correctly but discards the *character* of the design system. Three benchmarks are analyzed: a high-density data dashboard (compact, monochromatic), a consumer app (spacious, warm), and a developer tool (dark mode, precise). The synthesizer produces tokens that average the spacing scales, blend the palettes, and pick a middle-ground type size. The output is technically a valid design system, but it has no aesthetic identity — it looks like nothing.

**The specific failure in synthesis:** Most synthesis approaches treat design tokens as numeric data to average or range. They should be treated as *aesthetic expressions* that need a dominant voice and deliberate choices, not reconciliation.

**Warning signs:**
- Synthesizer prompt says "find the common values" rather than "establish a dominant aesthetic and note where benchmarks deviate"
- No concept of "hero benchmark" or weighted influence
- Spacing scale in output is a mix of 4px-grid and 5px-grid values (bleed from different benchmark philosophies)
- Style guide document produced reads like a list of values with no design rationale

**Prevention strategy:**
1. The synthesizer must first establish an aesthetic personality before merging values. Required synthesizer output fields: `aesthetic_summary` (narrative), `dominant_approach` (which benchmark's philosophy wins and why), `intentional_deviations` (where the synthesized system deliberately differs from any single benchmark).
2. Allow for user input on "primary benchmark" vs. "accent references." Even a simple config parameter that weights one benchmark more heavily produces dramatically better aesthetic coherence.
3. Spacing and type scales should be snapped to a *single* scale system during synthesis, not averaged. If benchmarks use 4px and 5px grids, the synthesizer must pick one.
4. The style guide document is the test of synthesis quality. If it reads like an AI-generated average rather than an intentional design stance, synthesis failed.

**Phase:** Synthesizer agent design, Style guide output design

---

## Pitfall 4: Token Names That Don't Survive Platform Translation

**Domain:** Token generation + multi-platform output (React/Tailwind + SwiftUI)

**What goes wrong:** Tokens are named using web conventions: `text-sm`, `bg-primary`, `border-radius-lg`. These names make intuitive sense in a Tailwind context. The SwiftUI generator then has to either use them verbatim (producing `Color.bgPrimary` which is semantically wrong in Swift) or re-derive semantic meaning from web-centric names (error-prone). The two platforms diverge immediately.

**The cascading failure:** Token name drift between platforms means the CLAUDE.md enforcement rules can't be written platform-agnostically. You end up with two separate sets of rules that drift over time.

**Warning signs:**
- Token file uses Tailwind utility class naming conventions (`text-base`, `space-4`) rather than semantic/design-token conventions (`typography-body-size`, `spacing-component-gap`)
- SwiftUI output has hand-mapped names that don't correspond to token names
- Platform-specific token files diverge in structure, not just syntax
- CLAUDE.md rules reference platform-specific names rather than canonical token names

**Prevention strategy:**
1. Define a platform-agnostic token naming convention before writing any generator code. Recommended: `{category}-{role}-{variant}` (e.g., `color-action-primary`, `spacing-layout-section`, `typography-heading-large`). This is the canonical name. Each platform generator translates canonical names to platform idioms.
2. Token output has three layers: (a) canonical JSON/YAML source of truth, (b) Tailwind config derived from canonical, (c) Swift extensions derived from canonical. Platform files are derived outputs, never the source.
3. CLAUDE.md rules reference canonical token names. Platform-specific sections of the rules show how canonical tokens map to platform idioms.
4. Test: given the same canonical token, both platform generators must produce a value that resolves to the same computed color/size/spacing. If they diverge, the pipeline is broken.

**Phase:** Token schema design (pre-implementation), Generator architecture design

---

## Pitfall 5: SwiftUI Output That Compiles But Isn't Idiomatic

**Domain:** Multi-platform output (SwiftUI)

**What goes wrong:** The SwiftUI generator produces code that compiles — Swift Color extensions exist, font extensions exist, spacing constants exist. But the generated code violates SwiftUI conventions: it doesn't use `@ScaledMetric` for accessibility-responsive spacing, uses raw `Color(hex:)` initializers instead of asset catalog integration, defines components that break on iOS 15 because they use APIs added in iOS 16, and produces View templates that are impossible to preview because they have hardcoded data.

**The deeper problem:** "Compiles" and "idiomatic" are different bars. A consumer who picks up the generated code and tries to use it discovers friction immediately — the code works in a demo but fails in production. "Drop-in ready" is violated.

**Warning signs:**
- Generator prompt doesn't specify minimum iOS version target
- Generated Color extensions use `Color(hex:)` custom initializer rather than named colors from asset catalog
- No `@ScaledMetric` usage in spacing definitions that affect touch targets or text containers
- Generated View files have no `#Preview` macro usage
- Type scale doesn't include Dynamic Type size mappings

**Prevention strategy:**
1. SwiftUI generation prompt must include a minimum iOS version target (default: iOS 16) and require the AI to stay within that API surface. Call out any API used that requires a later version.
2. For colors: generate both an asset catalog XML structure (Contents.json) and Swift Color extensions that reference those assets. Raw hex initializers are a code smell that signals the generator didn't account for dark mode and high-contrast accessibility.
3. For type: map each generated type style to a `UIFont.TextStyle` equivalent for Dynamic Type. Include this mapping in the generated style guide.
4. Every generated View component must include a `#Preview` block with realistic mock data.
5. Integration test: open the generated SwiftUI output in an Xcode project template and verify it builds with zero errors and zero warnings. Build failures or warnings in generated output are a hard reject.

**Phase:** SwiftUI generator design, Integration testing design

---

## Pitfall 6: Tailwind Config That Fights the Framework

**Domain:** Multi-platform output (React/Tailwind)

**What goes wrong:** The generator produces a `tailwind.config.js` that extends the default theme with custom values. But the custom values use non-standard scale keys (`primary`, `secondary` instead of numeric scales), override Tailwind defaults incompletely (adds `colors.brand` but doesn't clear `colors.blue` through `colors.zinc`), and uses CSS custom properties for some values but hardcoded hex for others. A developer using the generated config has access to both the design system tokens AND the full Tailwind default palette — the design system has no enforcement teeth.

**The specific pitfall with `extend` vs. replacement:** Most tutorials use `extend`, which adds to the default theme without removing it. A design system must replace, not extend, to actually constrain the token space.

**Warning signs:**
- Generated config uses `theme.extend` for color definitions rather than `theme.colors` (full replacement)
- Color scale uses non-numeric keys that break Tailwind utilities like `text-primary/50` (opacity modifier syntax requires numeric scales or specific setup)
- CSS custom property values in config but no corresponding CSS variables file generated
- No `content` paths configured — tree-shaking is broken out of the box

**Prevention strategy:**
1. Generated config must use `theme.colors` (full replacement) for the color palette, not `theme.extend.colors`. The purpose of the design system is to constrain choices, not add to them.
2. Use CSS custom properties throughout: the Tailwind config references `var(--color-brand-primary)`, and a generated `tokens.css` file defines the variables. This enables runtime theming and dark mode without config regeneration.
3. Scale keys for color must support Tailwind's opacity modifier syntax. Either use numeric keys (100-900) or configure `hsl` values with channel separation.
4. Integration test: generate a config, apply it to a fresh Vite/Next project, run `npx tailwindcss --dry-run`, verify no warnings about unused content paths or unknown values.

**Phase:** React/Tailwind generator design

---

## Pitfall 7: Parallel Analysis Agents Produce Incompatible Output Schemas

**Domain:** Claude Code sub-agent orchestration

**What goes wrong:** Three analysis agents run in parallel, each analyzing a different benchmark screenshot. Each agent is prompted to "analyze the design system." Without a rigidly enforced output schema, Agent A returns colors under a `palette` key, Agent B returns them under `colors`, and Agent C uses `colorSystem`. The synthesizer receives three documents with different structures and must either guess at mapping (error-prone) or fail.

**The less obvious variant:** Even with the same top-level keys, agents disagree on nested structure. Agent A returns `colors.primary` as a hex string; Agent B returns it as an object `{light: "#...", dark: "#..."}`. The synthesizer receives heterogeneous data under the same key and produces garbage or throws.

**Warning signs:**
- Analysis agent prompt describes what to extract but doesn't provide an output schema
- Output schema is described in prose rather than provided as a JSON schema or TypeScript type
- No validation step between analysis agent output and synthesizer input
- Synthesizer prompt handles "if the key is missing..." — absence of a key should be a hard error, not a handled case

**Prevention strategy:**
1. Define the analysis output schema as a JSON schema (or TypeScript type) before writing any agent prompts. This is not optional — it is the contract between analysis agents and the synthesizer.
2. Provide the schema to each analysis agent in its system prompt as a JSON template to fill in. Not a description — an actual template with placeholder values. The agent fills in values, it does not design the structure.
3. Add a validation step between analysis agent output and synthesizer input. Parse each agent's output against the schema. Reject and retry (with error context) if validation fails. Do not pass invalid data to the synthesizer.
4. The synthesizer prompt receives pre-validated, schema-conformant documents. Its job is semantic reconciliation, not structural normalization.

**Phase:** Agent architecture design (before writing any prompts), must be the first design decision made

---

## Pitfall 8: The Synthesizer Agent Context Window Overflows

**Domain:** Claude Code sub-agent orchestration

**What goes wrong:** Ten benchmarks are analyzed. Each analysis agent produces a detailed JSON document. The orchestrator collects all ten documents and passes them to the synthesizer agent in a single prompt. The combined documents exceed the context window. The synthesizer silently truncates input, synthesizes from an incomplete picture, and produces a design system that ignores several benchmarks.

**The silent failure variant:** Even within context limits, ten detailed documents produce a synthesizer prompt so long that the model's attention is diluted. Early-document tokens get less weight than late-document tokens. Benchmarks passed last have disproportionate influence.

**Warning signs:**
- No estimate of per-benchmark analysis output size in the architecture design
- Synthesizer receives raw analysis outputs with no compression/summarization step
- No maximum benchmark count enforced or documented
- Performance testing not planned for the maximum realistic input count

**Prevention strategy:**
1. Design a compression step between analysis and synthesis. Each analysis agent also produces a `summary` sub-document — a compact representation (max N tokens) of its key findings. The synthesizer works from summaries, not raw analyses.
2. Establish a maximum benchmark count with a documented rationale. For v1, 5-7 benchmarks is a reasonable upper bound given context constraints and diminishing returns on aesthetic diversity.
3. For counts above the threshold, implement hierarchical synthesis: group benchmarks into clusters, synthesize within clusters first, then synthesize across cluster results.
4. Log the total token count of synthesizer input during development. Set a hard ceiling (e.g., 50% of context window) and fail fast if exceeded.

**Phase:** Synthesizer agent design, Orchestrator design

---

## Pitfall 9: Generated CLAUDE.md Rules That Are Too Vague to Enforce

**Domain:** Claude Code skill output — CLAUDE.md rules generation

**What goes wrong:** The generated CLAUDE.md contains rules like "Always use the design system colors" and "Follow the typography scale." These are aspirational statements, not enforcement rules. A future Claude Code session reads them and doesn't know which colors, what the scale is, or what constitutes a violation. The rules have no teeth.

**The second failure mode:** Rules that are too specific — listing every hex value inline — bloat the CLAUDE.md to a size that dilutes attention. The rule about `#1A2B3C` on line 847 is never reached.

**Warning signs:**
- Rules use phrases like "use the design system" without defining what that means operationally
- No cross-references from CLAUDE.md rules to the generated token files
- Rules don't specify what constitutes a violation (what is forbidden, not just what is encouraged)
- Rules don't cover the "how to look it up" path — what should Claude do when uncertain about a token value?

**Prevention strategy:**
1. Each rule must be testable: a future Claude Code session should be able to answer "does this code violate this rule?" with yes/no. Rewrite any rule that produces "it depends" without further context.
2. Rules reference token names, not values. "Use `color-action-primary` for interactive elements" — not "use `#3B82F6` for buttons." The token file is the source of truth; the rule is the constraint.
3. Include a "how to find the right token" section in the CLAUDE.md: where the token files live, how they're named, and a few examples of correct usage per platform.
4. Include an explicit "do not use" section: raw hex values in SwiftUI files, hardcoded font sizes outside the type scale, spacing values not in the spacing token set. Prohibitions are easier to enforce than positive instructions.
5. Validate the rules by running a test session: open a fresh Claude Code session with only the generated CLAUDE.md, ask it to build a simple UI, and verify it reaches for the generated token files unprompted.

**Phase:** Rules generator design, Output validation design

---

## Pitfall 10: "Drop-In Ready" Means Different Things to Different Consumers

**Domain:** Output usability — all generators

**What goes wrong:** The tool produces output that is technically correct and well-organized. But "drop-in ready" was never operationally defined. For a React project: does it mean a Tailwind config only, or also component templates, or also a CSS reset that accounts for the generated token set? For SwiftUI: does it mean Swift files only, or also Xcode project modifications? The consumer drops the output into their project and immediately hits friction — missing integration steps, unclear file placement, no instructions for how to wire it up.

**Warning signs:**
- Output spec defines file contents but not file placement within a target project
- No "getting started" document generated alongside the design system artifacts
- Output tested only in isolation, not dropped into a real project template
- Different output targets have different levels of completeness (React is polished, SwiftUI is barebones)

**Prevention strategy:**
1. Define "drop-in ready" operationally for each platform target before writing any generator. Write the definition from the consumer's perspective: "A React developer should be able to X in Y minutes without Z additional steps."
2. Generate a `SETUP.md` alongside the design system artifacts. It describes: what files were generated, where to place each one in a standard project structure, what commands to run to activate the system, and what to expect after activation.
3. Test drop-in readiness with a real project: use the generated output in a fresh `create-next-app` (React) and a fresh Xcode project (SwiftUI). Document every friction point. Fix or document each one.
4. Enforce parity between platform targets. If the React output includes component templates, the SwiftUI output must include equivalent View templates. Asymmetric output quality is a product defect.

**Phase:** Output specification (pre-implementation), Integration testing

---

## Pitfall 11: Retry Logic That Masks Systematic Prompt Failures

**Domain:** Claude Code sub-agent orchestration

**What goes wrong:** An analysis agent fails to return valid JSON. The orchestrator retries. The agent fails again. The orchestrator retries a third time with the same prompt and the same image. On the third try, the agent returns something that passes JSON parsing but is semantically empty — it has the right structure but all values are defaults or placeholders. The orchestrator records this as success. The synthesizer receives a valid document that contributes nothing, and the design system is built on N-1 real benchmarks without the orchestrator or user knowing.

**The root cause:** Retry logic treats "returns parseable JSON" as success. It should treat "returns JSON that passes schema validation with non-default values" as success.

**Warning signs:**
- Retry logic validates structure only, not content
- No differentiation between transient failure (retry warranted) and systematic failure (prompt/image issue, don't retry blindly)
- Failed attempts are not logged with enough detail to diagnose the failure pattern
- User is not informed when a benchmark analysis produced low-confidence output

**Prevention strategy:**
1. Validation after retry must check both structure and content signals: are all required fields present, do values fall within reasonable ranges (e.g., color values are valid hex, font sizes are within 8-96px range), and do values differ from schema defaults?
2. Distinguish failure types: network/API failures (retry transparently), output validation failures (retry with augmented prompt that includes the validation error), and repeated validation failures (surface to user with the specific image and error, don't silently discard).
3. On final retry failure: do not discard silently. Include a `WARNINGS.md` in the output that lists which benchmarks produced low-confidence analysis and why. The user can re-run with a different image or adjust the design system manually.
4. Log all retry attempts with: attempt number, failure reason, image identifier. This makes systematic prompt problems diagnosable.

**Phase:** Orchestrator design, Error handling design

---

## Cross-Cutting Risks

These risks span multiple pitfalls and warrant explicit attention in planning:

| Risk | Manifestation | Mitigation |
|------|--------------|------------|
| Schema design deferred | Analysis/synthesizer schemas designed ad-hoc during implementation, producing mismatch | Define schemas first, before any prompt writing |
| "It worked on my benchmark" | Tool validated on one well-photographed benchmark, fails on real-world inputs (compressed PNGs, dark-mode screenshots, screenshots with UI overlays) | Test suite must include edge-case inputs: high compression, dark mode, partial screenshots, mixed DPI |
| SwiftUI treated as secondary | Implementation prioritizes React, SwiftUI added afterward with less rigor | Require SwiftUI output parity in the definition of done for every output milestone |
| Output not tested end-to-end | Each component (analysis, synthesis, generation) tested in isolation; integration never validated | Define an integration test from day one: full pipeline on a known benchmark with known expected output |
| CLAUDE.md rules decay | Rules generated correctly at v1, but as the skill evolves, rules output lags behind new token categories | Rules generation must be derived from token schema, not written separately — schema is the single source of truth |

---

*Research type: Project Research — Pitfalls dimension*
*Produced by: gsd-project-researcher*
*Consumed by: Roadmap/planning phase*
